<!DOCTYPE HTML>
<!--
    Dopetrope by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>
        <title>Laboratory for Progress</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="stylesheet" href="../../assets/css/main.css" />

    </head>
    <body class="homepage is-preload">
        <div id="page-wrapper">

            <section id="header-project">
                <h1><a href="https://progress.eecs.umich.edu/">Laboratory for Progress</a></h1>
                <p>Perceptive Robotics and Grounded Reasoning Systems</a></p>
            </section>

            <section id="main">
                <div class="container">

                    <!-- Content -->
                    <article class="box post project">
                        <header>
                            <h2>NARF: Neural Articulated Radiance Fields for Configuration-Aware Rendering</h2>
                            <p>Stanley Lewis, Jana Pavlasek, Odest Chadwicke Jenkins</p>
                        </header>

                        <!-- Pitch image. -->
                        <div class="image pitch centered"><img src="images/pitch.png" alt="" /></div>

                        <p>
                            Articulated objects pose a unique challenge for robotic perception and manipulation.
                            Their larger number of degrees-of-freedom makes localization and other tasks more computationaly difficult, while also making the process of real-world dataset collection unscalable.
                            With the aim of introducing a tool to address these scalability issues, we propose Neural Articulated Radiance Fields (NARF): a pipeline which uses a fully-differentiable, configuration parameterized Neural Radiance Field (NeRF) as a means of providing high quality renderings of articulated objects.
                            NARF requires no explicit knowledge of the object structure at inference time.
                            We propose a parts-based training mechanism which allows for creating object rendering models that generalize well across the configuration space even if the underlying training data has as few as one configuration represented.
                            We demonstrate the usefulness of this approach by training configurable renderers on a real-world articulated tool data collected via a Fetch Mobile Manipulator robot.
                            We then show the applicability of the model to generative or gradient-descent based inference methods by extracting configuration estimates and fine tuning object pose from an initial standard 6 degree-of-freedom rigid-body pose estimate.
                        </p>

                        <!-- Links. -->
                        <!-- <p> -->
                            <!-- <a href="#" class="paperlinks"><i class="far fa-file-pdf"></i> Read the Paper</a> &nbsp;&nbsp; -->
                            <!-- Optional. -->
                            <!-- <a href="#" class="paperlinks"><i class="fas fa-chalkboard-teacher"></i> Watch the Talk</a> -->
                        <!-- </p> -->

                    </article>

                    <!-- <article class="box post"> -->
                        <!-- <header> -->
                            <!-- <h2>Video</h2> -->
                        <!-- </header> -->

                        <!-- Link to video on YouTube. -->
                        <!-- To get your video link, go to your video and click "Share" -> "Embed" to get this iframe.
                             You can remove width and height. -->
                        <!-- <div class="video-container"> -->
                            <!-- <iframe class="video" src="https://www.youtube.com/embed/videoseries?list=PLDutmfAv2lfYZzuxA9BT9Y6rvCofTAGuc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
                        <!-- </div> -->
                    <!-- </article> -->

                    <!-- <article class="box post"> -->
                        <!-- <header> -->
                            <!-- <h2>Citation</h2> -->
                        <!-- </header> -->

                        <!-- The whitespace between <pre> tags will be preserved exactly how it is in the code. -->
                        <!-- <div class="citation"> -->
<!-- <pre><code>@inproceedings{tag, -->
  <!-- author = {Last, First and Last, First and ...}, -->
  <!-- title = {Paper title}, -->
  <!-- year = {2020}, -->
  <!-- ... -->
<!-- }</code></pre> -->
                        <!-- </div> -->
                    <!-- </article> -->

                <!-- </div> -->
            </section>

            <section id="footer">

                <!-- Copyright -->
                <div id="copyright">
                    <ul class="links">
                        <li>&copy; 2020. All rights reserved.</li><li>Design: Dopetrope by
                    <a href="http://twitter.com/ajlkn">AJ</a> for <a href="http://html5up.net/">HTML5 UP</li>
                    </ul>
                </div>

            </section>

        </div>

    </body>
</html>
